# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g9W5RpzGSKjyuuVrBa2y3DjDeXZF8X3i
"""

import numpy as np
import pandas as pd
from pandas import read_csv
import seaborn as sns
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error


# Load the training data
train=pd.read_csv('bb_2000s_train.csv')

#missing_values
#missing_values = train.isnull().any().sum()

#Data Exploration
train.keys()

y = train.loc[:,['Label']]
X = train[train.columns[3:15]]

# Load the test data
test =pd.read_csv('bb_2000s_test.csv')

print (train.head())
#print(train.head(30))

# distribution of PreviousHit
PreviousHit = [train['PreviousHit'].values]
sns.distplot(PreviousHit)

# distribution of anomalous features
anomalous_features = train.iloc[:,3:15].columns

plt.figure(figsize=(12,12*4))
gs = gridspec.GridSpec(12, 1)
for i, c in enumerate(train[anomalous_features]):
    ax = plt.subplot(gs[i])
    sns.distplot(train[c][train.Label == 1], bins=50)
    sns.distplot(train[c][train.Label == 0], bins=50)
    ax.set_xlabel('')
    ax.set_title('histogram of feature: ' + str(c))
plt.show()


#plotting distribution of continuous variables
train[train.dtypes[(train.dtypes=="float64")|(train.dtypes=="int64")].index.values].hist(figsize=[15,15])

#Data Splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#print (X_train)


#print(train.describe())
knn=KNeighborsClassifier(n_neighbors=5)
model = knn.fit(X_train[['Acousticness', 'Danceability','Energy','Instrumentalness', 'Key','Liveness', 'Loudness', 'Mode', 'PreviousHit', 'Speechiness', 'Tempo', 'Valence',]],y_train.values.ravel())

score = accuracy_score(y_test,knn.predict(X_test[['Acousticness', 'Danceability','Energy','Instrumentalness','Key','Liveness', 'Loudness', 'Mode', 'PreviousHit','Speechiness', 'Tempo', 'Valence',]]))
score

train.Label.value_counts()/train.Label.count()
L = train.shape[0]
ones = train[train['Label'] == 1]
zeros = train[train['Label'] == 0]
labels = ['zeros','ones']
classes = pd.value_counts(train['PreviousHit'], sort = True)
classes.plot(kind = 'bar', rot=0)
plt.title("frequency distribution")
plt.xticks(range(2), labels)
plt.xlabel("Label")
plt.ylabel("Frequency")

#Data Exploration
train.keys()

Years = train['Year'].tolist()
Danceability = train['Danceability'].tolist()
loudness = train['Loudness'].tolist()

plt.plot(Years, Danceability, '.')
plt.title("Danceability over the Decade")
plt.xlabel("Decade")
plt.ylabel("Danceability")
plt.xlim([2000,2011])

plt.plot(Years, loudness, '.')
plt.title("Loudness over the Decade")
plt.xlabel("Decade")
plt.ylabel("Loudness")
plt.xlim([2000,2011])

#Data Preprocessing
# Importing MinMaxScaler and initializing
from sklearn.preprocessing import MinMaxScaler
min_max=MinMaxScaler()
# Scaling down both train and test data set
X_train_minmax=min_max.fit_transform(X_train[['Acousticness', 'Danceability','Energy','Instrumentalness', 'Key','Liveness', 'Loudness', 'Mode', 'PreviousHit', 'Speechiness', 'Tempo', 'Valence']])
X_test_minmax=min_max.fit_transform(X_test[['Acousticness', 'Danceability','Energy','Instrumentalness', 'Key','Liveness', 'Loudness', 'Mode', 'PreviousHit', 'Speechiness', 'Tempo', 'Valence']])

knn_1=KNeighborsClassifier(n_neighbors=5)
knn_1.fit(X_train_minmax,y_train.values.ravel())
score = accuracy_score(y_test,knn_1.predict(X_test_minmax))
score

"""Why Normalization?
Normalization rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale.
"""

print(X_train_minmax)

# Standardizing the train and test data
from sklearn.preprocessing import scale

X_train_scale=scale(X_train[['Acousticness', 'Danceability','Energy','Instrumentalness', 'Key','Liveness', 'Loudness', 'Mode', 'PreviousHit', 'Speechiness', 'Tempo', 'Valence']])
X_test_scale=scale(X_test[['Acousticness', 'Danceability','Energy','Instrumentalness', 'Key','Liveness', 'Loudness', 'Mode', 'PreviousHit', 'Speechiness', 'Tempo', 'Valence']])

print(X_train_scale)
knn_2=KNeighborsClassifier(n_neighbors=15)
knn_2.fit(X_train_scale,y_train.values.ravel())
score = accuracy_score(y_test,knn_2.predict(X_test_scale))
score

"""Standardization is the process where the features are rescaled so that they’ll have the properties of a standard normal distribution with μ=0 and σ=1, where μ is the mean (average) and σ is the standard deviation from the mean. 

I used standardization because some variables such as Tempo and Loudness have a really large range could affect the weights of the variables.
"""

#Implementing KNN with multiple elements of k
neighbors = np.arange(1,20)
train_acc =np.empty(len(neighbors))
test_acc = np.empty(len(neighbors)) #test from data splitting not csv

for i,k in enumerate(neighbors):
  knn= KNeighborsClassifier(n_neighbors=k,algorithm="kd_tree",n_jobs=1)
  model = knn.fit(X_train_scale,y_train.values.ravel())
  train_acc[i] = knn.score(X_train_scale, y_train.values.ravel())
  test_acc[i] = knn.score(X_test_scale, y_test.values.ravel())
  
  

#finding the best K value by plotting accuracy of training set with testing set
plt.title('k-NN with different number of neighbors')
plt.plot(neighbors, test_acc, label='Testing Accuracy',color='black')
plt.plot(neighbors, train_acc, label='Training accuracy')
plt.legend()
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')
plt.show

"""This is PLOT A
The title of this plot is KNN accuracy with vatying number of neighbors.
"""

IPOidx = np.where(test_acc == max(test_acc))
x = neighbors[IPOidx]
x

#k_nearest_neighbours_classification
knn=KNeighborsClassifier( n_neighbors=7, algorithm='kd_tree', n_jobs=-1, weights= 'uniform' )
clf = knn.fit(X_train_scale,y_train.values.ravel())
#accuracy
score = accuracy_score(y_test,knn.predict(X_test_scale))

score

#confirming the accuracy with grid search
from sklearn.model_selection import GridSearchCV

#initialize the paremeter grid as dictionary
param_grid =  dict(n_neighbors=neighbors) 
#initialize search for best parameters using 10 fold cross validation
grid = GridSearchCV(knn, param_grid, cv=10, scoring = 'accuracy')
#fit the search object to validation dataset
grid_results = grid.fit(X_train_scale, y_train.values.ravel())
grid_results.best_score_
grid_results.best_params_
#param_grid

#Using best_parameter
knn_grid =KNeighborsClassifier( n_neighbors= 14, algorithm='kd_tree', n_jobs=-1, weights= 'uniform' )
clf_grid = knn_grid.fit(X_train_scale,y_train.values.ravel())
score = accuracy_score(y_test,knn_grid.predict(X_test_scale))
score

#AUC score for KNN
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
from sklearn.metrics import roc_curve, auc
probs = clf_grid.predict_proba(X_test_scale)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test,preds)
roc_auc = metrics.auc(fpr, tpr)
#threshold
roc_auc
#probs

#RMSE for KNN
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))

#Plotting ROC curve for KNN
plt.title('Receiver Operating Characteristic for KNN')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1], color='black', linestyle='--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""THIS IS PLOT B
NOTES
Why KNN with KDtree algorithm?
I.) KNN assumes similar feautures exist in close proximity.
II.) After standardizing the dataset, the feautures have similar ranges and scales making it easier to learn using nearest neighbors.
"""

#GRADIENT BOOSTING
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.metrics import mean_squared_error

#varrying tree depth 
d_values = np.arange(1, 20, dtype='int')

#
for d in d_values:
  xgb  =  XGBClassifier(booster= 'gbtree',eval_metric = 'auc',learning_rate = 0.1, n_estimators = 100,
                      max_depth = d,subsample = 0.9,colsample_bytree = 0.9 )
  xgb.fit(X_train_scale,y_train.values.ravel())
  #predictions on test set

#initialize the paremeter grid as dictionary
param_grid =  {'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]} 
#initialize search for best parameters using 10 fold cross validation
grid = GridSearchCV(xgb, param_grid, cv=10, scoring = 'accuracy')
#fit the search object to validation dataset
grid_results = grid.fit(X_train_scale, y_train.values.ravel())
grid_results.best_score_
grid_results.best_params_
#param_grid

# instantiating an XGBoost classifier with best selected d parameter
xgb_class  =  XGBClassifier( objective = 'reg:logistic', booster= 'gbtree',eval_metric = 'auc',learning_rate = 0.1, n_estimators = 100,
                      max_depth = 2,subsample = 0.9,colsample_bytree = 0.9 )
xgb_model = xgb_class.fit(X_train_scale,y_train.values.ravel())
#RMSE
preds = xgb_class.predict(X_test_scale)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))

#Accuracy of XGBoost
score = accuracy_score(y_test,xgb_class.predict(X_test_scale))
score

#AUC score for XGBOOST
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
from sklearn.metrics import roc_curve, auc
probs = xgb_class.predict_proba(X_test_scale)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test,preds)
roc_auc_XGB = metrics.auc(fpr, tpr)
#threshold
roc_auc_XGB
#probs

#Plotting ROC curve for XGBOOST CLassifier
plt.title('Receiver Operating Characteristic for XGBOOST')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_XGB)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1], color='b', linestyle='--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""THIS IS PLOT C
We used XGBoost because of the following reasons:
I.) XGBoost has regularization that helps reduce overfitting
II.) XGBoost converts weak learners to strong learners by amassing many weak learners into strong learners, thus improving the learning process.
III.) The xgboost is a very powerful algorithm as it was built for computational speed.
IV.) The Xgboost allows the user run a cross validation at each iteration of the boosting process.
"""



SVM_model=SVC()
SVM_model.fit(X_train_scale,y_train.values.ravel())

C_range = np.arange(-3.0, 6.0, 1.0)
C_values = np.power(10.0, C_range)

gamma_range = np.arange(-2.0, 4.0, 1.0)
gamma_values = np.power(10.0, gamma_range)

SVM_param_grid = {'gamma':gamma_values,'C':C_values}

SVM_tuned = GridSearchCV(SVM_model,param_grid=SVM_param_grid,cv=10).fit(X_test_scale,y_test)

#RMSE
pred_svm = SVM_tuned.predict(X_test_scale)
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))

all_model_score = {
    'SVM': accuracy_score(y_test,SVM_tuned.predict(X_test_scale)),
    'KNN': accuracy_score(y_test,knn.predict(X_test_scale)),
    'XGBoost': accuracy_score(y_test,xgb_class.predict(X_test_scale)),    
}

print(all_model_score)

#STandardizing Test Set

test_scale=scale(test[['Acousticness', 'Danceability','Energy','Instrumentalness', 'Key','Liveness', 'Loudness', 'Mode', 'PreviousHit', 'Speechiness', 'Tempo', 'Valence']])

preds = xgb_class.predict(test_scale)
print(preds)